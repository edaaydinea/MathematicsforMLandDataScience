{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 3 - Sampling and Point estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 1 - Population and Sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Population and Sample\n",
    "\n",
    "\n",
    "1. **Population and Sample**:\n",
    "   - **Population**: The entire group of individuals or items that you want to study. Represented by \\( N \\).\n",
    "     - Example: All people in Statistopia (10,000 individuals).\n",
    "   - **Sample**: A smaller subset of the population that is actually observed or measured. Represented by \\( n \\).\n",
    "     - Example: A random selection of 100 individuals from Statistopia.\n",
    "\n",
    "2. **Sampling Strategies**:\n",
    "   - **Random Sampling**: The best method to ensure an unbiased sample. Each individual in the population has an equal chance of being selected.\n",
    "   - **Non-Random Sampling**: Should be avoided as it may lead to biased results (e.g., selecting the shortest individuals when measuring height).\n",
    "\n",
    "3. **Independent and Identically Distributed (i.i.d.) Samples**:\n",
    "   - **Independent Samples**: Each sample should be independent of others. The selection of one individual should not affect the selection of another.\n",
    "   - **Identically Distributed**: The rule used to select the sample should be consistent across all selections to maintain uniformity.\n",
    "\n",
    "4. **Importance in Machine Learning**:\n",
    "   - **Data Sets as Samples**: In ML, every dataset is a sample of the possible data. No matter how large, it does not represent the entire population.\n",
    "   - **Representative Data Sets**: Ensuring that your dataset is representative of the population's distribution is crucial for model accuracy.\n",
    "     - Example: For cat image classification, the dataset should include diverse images (cats on grass, couches, etc.) to avoid biased learning.\n",
    "\n",
    "5. **Formal Definitions**:\n",
    "   - **Population**: The entire set of individuals or elements under study, sharing common characteristics.\n",
    "   - **Sample**: A subset of the population used to draw conclusions about the entire population.\n",
    "   - **N**: Denotes the population size.\n",
    "   - **n**: Denotes the sample size.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample Mean\n",
    "\n",
    "\n",
    "1. **Population Mean ($\\mu$)**:\n",
    "   - The average value of a characteristic (e.g., height) in the entire population.\n",
    "   - Example: In Statistopia, if the population size is 10, and the average height is 160 cm, $\\mu = 160$ cm.\n",
    "\n",
    "2. **Sample Mean ($\\bar{X}$)**:\n",
    "   - The average value calculated from a subset of the population.\n",
    "   - Represented by $\\bar{X}_1$, $\\bar{X}_2$, etc., for different samples.\n",
    "   - Example: For a sample of 6 people, $\\bar{X}_1 = 160.97$ cm.\n",
    "\n",
    "3. **Sample Size ($n$)**:\n",
    "   - The number of individuals in the sample. Larger sample sizes generally provide better estimates of the population mean.\n",
    "   - Example: A sample of size 6 ($n=6$) generally gives a better estimate of $\\mu$ than a sample of size 2 ($n=2$).\n",
    "\n",
    "4. **Sampling and Estimation**:\n",
    "   - **Random Sampling**: Essential for getting a representative estimate of the population mean.\n",
    "     - Example: If a sample is formed by the shortest individuals, it might not be a good estimate.\n",
    "   - **Estimate Accuracy**: The accuracy of the sample mean as an estimate of the population mean improves with larger sample sizes.\n",
    "     - Example: $\\bar{X}_1$ (with $n=6$) is a better estimate than $\\bar{X}_3$ (with $n=2$).\n",
    "\n",
    "5. **Estimating Variance**:\n",
    "   - When estimating the population variance using a sample, the calculated sample variance will typically be close to the population variance but not exactly the same.\n",
    "   - This discrepancy occurs because the sample variance tends to underestimate the population variance. Later lessons will explore this in more detail.\n",
    "\n",
    "6. **Key Takeaway**:\n",
    "   - The larger the sample size ($n$), the better the estimate of the population mean ($\\mu$) and variance you will obtain from the sample.\n",
    "\n",
    "#### Notes for Exam Preparation:\n",
    "- Understand the difference between the population mean ($\\mu$) and the sample mean ($\\bar{X}$).\n",
    "- Recognize how sample size ($n$) impacts the accuracy of estimating $\\mu$.\n",
    "- Remember that while sample variance is close to the population variance, it often slightly underestimates it.\n",
    "- Be aware that random sampling is crucial for obtaining a good estimate of the population parameters.\n",
    "\n",
    "This summary outlines the essential points about estimating population means and variance using samples, including the importance of sample size and the concept of random sampling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample Proportion\n",
    "\n",
    "1. **Population Proportion (P)**:\n",
    "   - The proportion of the entire population that has a specific characteristic.\n",
    "   - Calculated as $ P = \\frac{x}{N} $, where $ x $ is the number of individuals with the characteristic and $ N $ is the total population size.\n",
    "   - **Example**: In Statistopia, if 4 out of 10 people own a bicycle, the population proportion $ P = \\frac{4}{10} = 0.4 $ or 40%.\n",
    "\n",
    "2. **Sample Proportion ($\\hat{P}$)**:\n",
    "   - The proportion of a characteristic observed in a randomly selected sample from the population.\n",
    "   - Calculated as $ \\hat{P} = \\frac{x_{\\text{sample}}}{n} $, where $ x_{\\text{sample}} $ is the number of individuals with the characteristic in the sample, and $ n $ is the sample size.\n",
    "   - **Example**: If 2 out of 6 randomly sampled people own a bicycle, the sample proportion $ \\hat{P} = \\frac{2}{6} = 0.333 $ or 33.3%.\n",
    "\n",
    "3. **Relationship Between P and $\\hat{P}$**:\n",
    "   - The sample proportion ($\\hat{P}$) serves as an estimate of the population proportion (P).\n",
    "   - While $\\hat{P}$ may not exactly equal $ P $, it provides a useful approximation, especially when a large and representative sample is used.\n",
    "\n",
    "4. **Key Takeaway**:\n",
    "   - Population proportion (P) gives the true proportion of a characteristic within the entire population.\n",
    "   - Sample proportion ($\\hat{P}$) is an estimate of $ P $ based on a subset of the population and may vary depending on the sample size and representativeness.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample Variance\n",
    "\n",
    "This explanation covers the concepts of population variance and sample variance, and how to estimate them when working with a sample instead of the entire population. Here's a summary:\n",
    "\n",
    "1. **Population Variance (σ²):**\n",
    "   - Measures how spread out data points are around the population mean (μ).\n",
    "   - Formula: $\\sigma^2 = \\frac{1}{N} \\sum_{i=1}^{N} (x_i - \\mu)^2$, where $N$ is the population size.\n",
    "\n",
    "2. **Sample Variance (s²):**\n",
    "   - Used when you only have a sample of the population.\n",
    "   - The sample mean ($\\bar{x}$) replaces the population mean, and $n$ (sample size) replaces $N$.\n",
    "   - Initial estimation: $ \\hat{\\sigma}^2 = \\frac{1}{n} \\sum_{i=1}^{n} (x_i - \\bar{x})^2 $. However, this formula introduces a bias because the sample mean is used instead of the population mean.\n",
    "\n",
    "3. **Bias Correction with $n-1$:**\n",
    "   - To correct the bias, divide by $n-1$ instead of $n$, resulting in an unbiased estimator of the population variance.\n",
    "   - Formula: $ s^2 = \\frac{1}{n-1} \\sum_{i=1}^{n} (x_i - \\bar{x})^2 $.\n",
    "   - This adjustment is necessary because using the sample mean tends to underestimate the variance.\n",
    "\n",
    "4. **Impact of Sample Size:**\n",
    "   - The difference between dividing by $n$ and $n-1$ becomes less significant as the sample size increases. For small samples, the difference is more noticeable.\n",
    "\n",
    "5. **Alternative Variance Estimators:**\n",
    "   - Some statistical techniques, like maximum likelihood estimation, use the $ \\hat{\\sigma}^2 = \\frac{1}{n} \\sum_{i=1}^{n} (x_i - \\bar{x})^2 $ formula, which has a small bias.\n",
    "   - Despite this, the $s^2$ estimator (using $n-1$) is the most common and preferred method in practice for estimating variance from a sample.\n",
    "\n",
    "In summary, when estimating variance from a sample, using the corrected formula with $n-1$ provides a more accurate (unbiased) estimate of the population variance. This approach is standard in most statistical analyses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Law of Large Numbers\n",
    "\n",
    "The Law of Large Numbers is a fundamental concept in probability and statistics that states that as the size of a sample drawn from a population increases, the sample mean (or average) will get closer and closer to the population mean.\n",
    "\n",
    "### Example with Human Height:\n",
    "Let's say you want to estimate the average height of all humans. If you measure the height of just one person, you get an estimate, but it's likely to be off due to randomness. If you measure two or three people and take the average of their heights, your estimate improves. As you increase the number of people measured—say 10, 100, or 1,000—the average of these heights will get closer to the true average height of the entire human population.\n",
    "\n",
    "### Dice Example:\n",
    "Imagine you have a fair 4-sided die with possible outcomes of 1, 2, 3, and 4. The true average (mean) outcome of a single roll is 2.5. To illustrate the Law of Large Numbers, let’s consider an experiment where you roll the die twice and calculate the average of the two rolls. There are 16 possible pairs of outcomes (e.g., (1,1), (1,2), etc.), and the average for each pair is listed in a table.\n",
    "\n",
    "If you take just one sample (e.g., rolling a 4 and a 3), the average might be 3.5—higher than the true mean of 2.5. But if you continue taking more samples and calculating the average each time, you'll notice that these averages begin to converge towards 2.5.\n",
    "\n",
    "### Law of Large Numbers:\n",
    "Mathematically, the Law of Large Numbers states that if you take an increasing number of independent and identically distributed (i.i.d) samples $ X_1, X_2, ..., X_n $ from a population, the average of these samples:\n",
    "\n",
    "$$\\bar{X}_n = \\frac{X_1 + X_2 + ... + X_n}{n}$$\n",
    "\n",
    "will tend to get closer and closer to the population mean $ \\mu_X $ as the sample size $ n $ increases.\n",
    "\n",
    "### Conditions:\n",
    "1. **Random Sampling**: The samples must be drawn randomly from the population.\n",
    "2. **Large Sample Size**: The sample size must be sufficiently large. Larger samples provide more accurate estimates.\n",
    "3. **Independence**: The individual observations in the sample must be independent of each other.\n",
    "\n",
    "In summary, the Law of Large Numbers ensures that as you collect more data, your estimate of the population mean becomes increasingly accurate, assuming the conditions are met."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Central Limit Theorem - Discrete Random Variable\n",
    "\n",
    "The concept you're describing is the **Central Limit Theorem (CLT)**, one of the most important results in statistics. It states that the distribution of the sum (or average) of a large number of independent, identically distributed (i.i.d.) random variables tends to become a normal distribution, even if the original variables themselves are not normally distributed. This is true regardless of the original distribution, as long as the number of variables (n) is sufficiently large.\n",
    "\n",
    "### Example with Coin Flips:\n",
    "Let's take the example of flipping a fair coin, where the probability of getting heads (X = 1) is 0.5 and tails (X = 0) is also 0.5. For a single flip, the probability distribution is simple: there's a 50% chance of getting either heads or tails.\n",
    "\n",
    "Now, consider flipping two coins. The possible outcomes are:\n",
    "- 0 heads (TT) with a probability of 1/4,\n",
    "- 1 head (HT or TH) with a probability of 2/4,\n",
    "- 2 heads (HH) with a probability of 1/4.\n",
    "\n",
    "This gives us a distribution that's starting to look more complex, but it's still discrete.\n",
    "\n",
    "As you increase the number of flips:\n",
    "- With three coins, the distribution has more outcomes (0, 1, 2, or 3 heads) and looks more spread out.\n",
    "- With 10 coins, the distribution of the number of heads becomes more bell-shaped and starts resembling a normal distribution.\n",
    "\n",
    "### Why Does This Happen?\n",
    "Each coin flip is a Bernoulli trial, which is a simple random variable that can take one of two values: 1 (for heads) or 0 (for tails). When you sum the results of multiple coin flips, you're essentially adding up a number of Bernoulli random variables. The CLT states that as you increase the number of these trials (coin flips), the distribution of their sum (or average) will approach a normal distribution.\n",
    "\n",
    "### Calculating the Mean and Variance:\n",
    "For a single coin flip (n = 1):\n",
    "- The mean $\\mu$ is $np = 1 \\times 0.5 = 0.5$.\n",
    "- The variance $\\sigma^2$ is $np(1-p) = 1 \\times 0.5 \\times 0.5 = 0.25$.\n",
    "\n",
    "As you increase the number of flips (n):\n",
    "- The mean becomes $\\mu = np$.\n",
    "- The variance becomes $\\sigma^2 = np(1-p)$.\n",
    "\n",
    "For example:\n",
    "- For 10 flips, $\\mu = 10 \\times 0.5 = 5$ and $\\sigma^2 = 10 \\times 0.5 \\times 0.5 = 2.5$.\n",
    "\n",
    "### Implication of the Central Limit Theorem:\n",
    "The Central Limit Theorem is powerful because it allows us to make inferences about the sum or average of a large number of independent random variables, regardless of their original distribution. This is why the normal distribution appears so frequently in statistical analyses, even when the underlying data is not normally distributed.\n",
    "\n",
    "In summary, the Central Limit Theorem shows that as you increase the number of observations or trials, the distribution of their average will tend to become normal (Gaussian), with a mean of $np$ and a variance of $np(1-p)$, making it a cornerstone of probability theory and statistics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Central Limit Theorem - Continuous Random Variable\n",
    "\n",
    "The example you provided effectively illustrates the **Central Limit Theorem (CLT)** through a practical experiment. The CLT is a cornerstone of statistics, and this example highlights how the theorem applies to a continuous random variable. Here's a summary of the key points:\n",
    "\n",
    "1. **Central Limit Theorem (CLT):** It states that when you take a large enough sample size (typically n ≥ 30) from any distribution, the distribution of the sample mean will tend to be normally distributed, regardless of the original distribution's shape. The larger the sample size, the more the distribution of the sample mean will resemble a Gaussian (normal) distribution.\n",
    "\n",
    "2. **Example Using Call Wait Time:**\n",
    "   - **Initial Setup:** The wait time for a call to be answered follows a uniform distribution between 0 and 15 minutes.\n",
    "   - **Sampling and Averaging:** The experiment involves taking different sample sizes (n = 1, 2, 3, etc.) and calculating the average wait time for each sample. Repeating this process many times and plotting the resulting averages provides a histogram of the sample means.\n",
    "   - **Observations:**\n",
    "     - For **n = 1**: The distribution of the sample mean resembles the original uniform distribution.\n",
    "     - For **n = 2**: The distribution begins to take on a triangular shape, centered around the population mean (7.5 minutes).\n",
    "     - For **n = 3** and beyond: The distribution starts to become more bell-shaped and symmetric, approaching the normal distribution.\n",
    "   - **Mean and Variance:** \n",
    "     - The mean of the sample means remains constant at the population mean (7.5 minutes).\n",
    "     - The variance of the sample means decreases as n increases, showing that the sample mean becomes more concentrated around the population mean.\n",
    "\n",
    "3. **Standardization:** \n",
    "   - To compare distributions of sample means for different n, standardization is used. By standardizing, you transform the distribution into a standard normal distribution (mean = 0, variance = 1).\n",
    "   - This makes it easier to observe the effects of the CLT, as the distribution of the sample means becomes closer to a standard normal distribution as n increases.\n",
    "\n",
    "4. **Formal Definition of CLT:** \n",
    "   - As n approaches infinity, the standardized average of n independent and identically distributed (i.i.d.) random variables will follow a standard normal distribution.\n",
    "   - Alternatively, the sum of these variables, after proper scaling, will also follow a standard normal distribution.\n",
    "\n",
    "5. **Practical Application:** \n",
    "   - In practice, the CLT allows statisticians to make inferences about population parameters using sample data. The fact that the sample mean follows a normal distribution (for sufficiently large n) makes it easier to calculate confidence intervals and conduct hypothesis tests, even when the original data is not normally distributed.\n",
    "\n",
    "This experiment and explanation show how the CLT is applicable in real-world situations, making it an essential tool in statistics for analyzing and interpreting data from various distributions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 2 - Point Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Point Estimation\n",
    "\n",
    "In this lesson, you’re diving into the important concept of **estimation**, which is central to statistics and machine learning. Here’s a breakdown of the key ideas you'll be exploring:\n",
    "\n",
    "#### 1. **Estimation in Statistics:**\n",
    "   - **Point Estimation:** This involves estimating an unknown parameter of a population (like the mean or variance) using sample data. A point estimate provides a single value as an estimate of the parameter.\n",
    "   - **Maximum Likelihood Estimation (MLE):** MLE is one of the most widely used methods for point estimation. It aims to find the parameter values that maximize the likelihood function, meaning they make the observed data most probable. MLE is crucial in various machine learning models, including logistic regression, neural networks, and more.\n",
    "\n",
    "#### 2. **Maximum a Posteriori (MAP) Estimation:**\n",
    "   - MAP is an extension of MLE that incorporates prior knowledge or beliefs about the parameter, using Bayes' theorem. While MLE only considers the likelihood of the data, MAP combines this likelihood with a prior distribution, resulting in an estimate that reflects both the data and prior beliefs.\n",
    "   - **Bayes' Theorem in MAP:** Bayes' theorem updates the probability estimate for a parameter as more evidence (data) becomes available. The MAP estimate is the mode of the posterior distribution, which is the product of the likelihood function and the prior distribution.\n",
    "   - **MAP vs. MLE:** MAP can be seen as a regularized version of MLE. In machine learning, regularization is a technique used to prevent overfitting by adding a penalty for complexity (e.g., large coefficients in a regression model). MAP estimation introduces this regularization naturally through the prior distribution.\n",
    "\n",
    "#### 3. **Regularization in Machine Learning:**\n",
    "   - **Overfitting Prevention:** Regularization techniques, such as L1 (lasso) and L2 (ridge) regularization, are used in machine learning to prevent models from fitting the noise in the data too closely, which leads to poor generalization to new data.\n",
    "   - **Connection to MAP:** The lesson will show you how MAP estimation can be interpreted as MLE with a regularization term, where the prior distribution in MAP serves as the source of regularization. This perspective is not only elegant but also practically useful in building robust machine learning models.\n",
    "\n",
    "#### 4. **Detailed Walkthrough:**\n",
    "   - The lesson will guide you through the mathematical details of MLE and MAP estimation, demonstrating how these methods work in practice.\n",
    "   - You’ll learn how to derive MLE and MAP estimates, understand their properties, and see examples of their application in real-world problems.\n",
    "\n",
    "This lesson sets the stage for understanding the core techniques used in statistical inference and machine learning, particularly in how models are trained and optimized to make predictions based on data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maximum Likelihood Estimation Motivation\n",
    "\n",
    "In this video, you're learning about **Maximum Likelihood Estimation (MLE)**, a fundamental concept in statistics and machine learning that's used to train models by finding the most probable explanation for observed data. Let's break down the key points:\n",
    "\n",
    "#### **Concept of MLE:**\n",
    "- **Scenario Setup:** MLE is about inferring the most likely scenario that explains a given set of evidence. For instance, you see popcorn on the floor and want to determine what led to this. You consider different scenarios, like people watching a movie, playing board games, or someone taking a nap.\n",
    "- **Probability Assessment:** Each scenario has a different probability of leading to popcorn on the floor:\n",
    "  - **Movies:** High probability\n",
    "  - **Board games:** Medium probability\n",
    "  - **Taking a nap:** Low probability\n",
    "\n",
    "- **Choosing the Most Likely Scenario:** Since watching a movie has the highest probability of resulting in popcorn on the floor, you infer that this scenario is the most likely to have occurred. This process of choosing the scenario that maximizes the probability of the observed evidence is known as **maximum likelihood**.\n",
    "\n",
    "#### **Application to Machine Learning:**\n",
    "- **Model Selection:** In machine learning, you have a dataset (your evidence) and several possible models (scenarios) that could have generated this data. MLE helps you choose the model that most likely produced the data.\n",
    "- **Maximizing Conditional Probability:** You estimate the probability of observing your data given each model (i.e., P(Data | Model 1), P(Data | Model 2), etc.). The model that maximizes this probability is considered the best fit.\n",
    "  \n",
    "#### **Link to Linear Regression:**\n",
    "- **Data Points and Models:** Imagine you have data points and three possible linear models (lines) that could explain these points.\n",
    "- **Probability of Data Given a Model:** MLE would involve evaluating how likely it is that the data points were generated by each of these lines. The line that makes the observed data most probable is the one you choose.\n",
    "- **Linear Regression Context:** In linear regression, you're finding the line that best fits your data by maximizing the likelihood of the data given the line (or minimizing the sum of squared errors, which is closely related).\n",
    "\n",
    "#### **Summary:**\n",
    "- **MLE in Simple Terms:** MLE involves picking the scenario or model that makes the observed data most likely. In machine learning, it helps you choose the model that best explains the data you have.\n",
    "- **Why It's Important:** MLE is a cornerstone of many machine learning algorithms, including linear regression, where it helps in selecting the best model to predict future outcomes based on past data.\n",
    "\n",
    "You'll delve into more detailed examples and mathematical explanations of MLE later, but this gives you a solid conceptual foundation to understand how MLE works and why it's so widely used in machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLE: Bernoulli Example\n",
    "\n",
    "Let's break down the MLE example you provided with the coin tosses to understand the concept more clearly.\n",
    "\n",
    "#### **Scenario:**\n",
    "- **Coin Tosses:** You tossed a coin 10 times, resulting in 8 heads and 2 tails.\n",
    "- **Coins:** You have three possible coins:\n",
    "  - **Coin 1:** Probability of heads = 0.7\n",
    "  - **Coin 2:** Probability of heads = 0.5 (fair coin)\n",
    "  - **Coin 3:** Probability of heads = 0.3\n",
    "\n",
    "#### **Objective:**\n",
    "Determine which coin most likely produced the observed results (8 heads, 2 tails).\n",
    "\n",
    "#### **Maximum Likelihood Estimation (MLE):**\n",
    "1. **Calculate Likelihood for Each Coin:**\n",
    "   - **Coin 1:**\n",
    "     - Probability of 8 heads and 2 tails = $0.7^8 \\times 0.3^2$\n",
    "     - Compute: $0.7^8 \\approx 0.0576$, $0.3^2 = 0.09$\n",
    "     - Likelihood = $0.0576 \\times 0.09 = 0.0051$\n",
    "\n",
    "   - **Coin 2:**\n",
    "     - Probability of 8 heads and 2 tails = $0.5^{10}$\n",
    "     - Compute: $0.5^{10} = 0.0010$\n",
    "\n",
    "   - **Coin 3:**\n",
    "     - Probability of 8 heads and 2 tails = $0.3^8 \\times 0.7^2$\n",
    "     - Compute: $0.3^8 \\approx 0.000656$, $0.7^2 = 0.49$\n",
    "     - Likelihood = $0.000656 \\times 0.49 = 0.00032$\n",
    "\n",
    "   The highest likelihood is for **Coin 1** (0.0051), so it’s the most probable coin that generated the data.\n",
    "\n",
    "2. **Finding the Optimal Probability $P$:**\n",
    "   - Suppose we don't know the exact probability of heads $P$, but we can use MLE to estimate it.\n",
    "   - The likelihood function for $P$ (where $P$ is the probability of heads) is:\n",
    "     $$   \\text{Likelihood}(P) = P^8 \\times (1 - P)^2$$\n",
    "   - To simplify, use the **log likelihood**:\n",
    "     $$   \\text{Log Likelihood} = 8 \\log(P) + 2 \\log(1 - P)$$\n",
    "   - Take the derivative of the log likelihood with respect to $P$ and set it to 0:\n",
    "     $$   \\frac{d}{dP} [8 \\log(P) + 2 \\log(1 - P)] = \\frac{8}{P} - \\frac{2}{1 - P} = 0$$\n",
    "   - Solve for $P$:\n",
    "     $$   \\frac{8}{P} = \\frac{2}{1 - P}$$\n",
    "     $$   8(1 - P) = 2P$$\n",
    "     $$   8 - 8P = 2P$$\n",
    "     $$   8 = 10P$$\n",
    "     $$   P = \\frac{8}{10} = 0.8$$\n",
    "\n",
    "   The estimated probability of heads $P$ is 0.8, which aligns with the observed data (8 heads in 10 tosses).\n",
    "\n",
    "#### **General Case:**\n",
    "- **Bernoulli Variable:** For a sequence of $n$ coin flips with $k$ heads, the likelihood function is:\n",
    "  $$\\text{Likelihood}(P) = P^k \\times (1 - P)^{n - k}$$\n",
    "- **Log Likelihood:**\n",
    "  $$\\text{Log Likelihood} = k \\log(P) + (n - k) \\log(1 - P)$$\n",
    "- **Maximization:** Taking the derivative and solving gives:\n",
    "  $$\\hat{P} = \\frac{k}{n}$$\n",
    "  Thus, the MLE for the probability of heads is simply the proportion of heads observed in the flips.\n",
    "\n",
    "This example illustrates how MLE can be used to estimate parameters (like the probability of heads) by maximizing the likelihood of observing the given data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLE: Gaussian Example\n",
    "\n",
    "#### **Understanding Maximum Likelihood Estimation (MLE) with Gaussian Distributions**\n",
    "\n",
    "In this scenario, you're given some observations (1 and -1) and need to determine which of the given Gaussian distributions most likely generated these observations.\n",
    "\n",
    "#### **Step-by-Step Solution:**\n",
    "\n",
    "1. **Initial Choices:**\n",
    "   - **Distribution 1:** Normal distribution with mean 10 and standard deviation 1\n",
    "   - **Distribution 2:** Normal distribution with mean 2 and standard deviation 1\n",
    "\n",
    "   To determine which distribution is more likely to have generated the observations, calculate the likelihood of observing 1 and -1 under each distribution.\n",
    "\n",
    "2. **Calculate Likelihoods:**\n",
    "\n",
    "   - **Distribution 1:** Mean = 10, SD = 1\n",
    "     - Likelihood of observing 1: $\\phi(1; 10, 1)$\n",
    "     - Likelihood of observing -1: $\\phi(-1; 10, 1)$\n",
    "\n",
    "     Using the normal density function:\n",
    "     $$\\phi(x; \\mu, \\sigma) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp \\left( -\\frac{(x - \\mu)^2}{2\\sigma^2} \\right)$$\n",
    "     For mean 10, SD 1:\n",
    "     - $\\phi(1; 10, 1) \\approx \\frac{1}{\\sqrt{2\\pi}} \\exp \\left( -\\frac{(1 - 10)^2}{2} \\right)$\n",
    "     - $\\phi(-1; 10, 1) \\approx \\frac{1}{\\sqrt{2\\pi}} \\exp \\left( -\\frac{(-1 - 10)^2}{2} \\right)$\n",
    "\n",
    "   - **Distribution 2:** Mean = 2, SD = 1\n",
    "     - Likelihood of observing 1: $\\phi(1; 2, 1)$\n",
    "     - Likelihood of observing -1: $\\phi(-1; 2, 1)$\n",
    "\n",
    "     For mean 2, SD 1:\n",
    "     - $\\phi(1; 2, 1) \\approx \\frac{1}{\\sqrt{2\\pi}} \\exp \\left( -\\frac{(1 - 2)^2}{2} \\right)$\n",
    "     - $\\phi(-1; 2, 1) \\approx \\frac{1}{\\sqrt{2\\pi}} \\exp \\left( -\\frac{(-1 - 2)^2}{2} \\right)$\n",
    "\n",
    "   You’ll find that the second distribution (mean = 2, SD = 1) provides higher likelihoods for both observations 1 and -1 compared to the first distribution.\n",
    "\n",
    "3. **Compare Likelihoods:**\n",
    "\n",
    "   For the three new Gaussians:\n",
    "   - **Gaussian 1:** Mean = -1, SD = 1\n",
    "   - **Gaussian 2:** Mean = 0, SD = 1\n",
    "   - **Gaussian 3:** Mean = 1, SD = 1\n",
    "\n",
    "   Calculate the likelihood of observing 1 and -1 for each Gaussian:\n",
    "\n",
    "   - **Gaussian 1:** Mean = -1, SD = 1\n",
    "     - Likelihoods: $\\phi(1; -1, 1) \\approx 0.242$ and $\\phi(-1; -1, 1) \\approx 0.242$\n",
    "     - Product of likelihoods: $0.242 \\times 0.242 = 0.059$\n",
    "\n",
    "   - **Gaussian 2:** Mean = 0, SD = 1\n",
    "     - Likelihoods: $\\phi(1; 0, 1) \\approx 0.398$ and $\\phi(-1; 0, 1) \\approx 0.398$\n",
    "     - Product of likelihoods: $0.398 \\times 0.398 = 0.159$\n",
    "\n",
    "   - **Gaussian 3:** Mean = 1, SD = 1\n",
    "     - Likelihoods: $\\phi(1; 1, 1) \\approx 0.398$ and $\\phi(-1; 1, 1) \\approx 0.054$\n",
    "     - Product of likelihoods: $0.398 \\times 0.054 = 0.022$\n",
    "\n",
    "   **Conclusion:** The Gaussian distribution with mean 0 and standard deviation 1 has the highest product of likelihoods, making it the most likely distribution to have generated the observations 1 and -1.\n",
    "\n",
    "4. **Variance and Standard Deviation:**\n",
    "\n",
    "   - **Variance of Observations:** $\\text{Var} = \\frac{(1 - \\text{mean})^2 + (-1 - \\text{mean})^2}{2}$. For the observations (1, -1), the mean is 0, so variance = 1.\n",
    "   - **Distributions Variance:**\n",
    "     - SD = 0.5 → Variance = 0.25\n",
    "     - SD = 1 → Variance = 1\n",
    "     - SD = 2 → Variance = 4\n",
    "\n",
    "   The distribution with SD 1 (variance 1) matches the variance of the observations, which is why it is preferred.\n",
    "\n",
    "#### **Summary:**\n",
    "\n",
    "- **Gaussian Distribution with Mean 0 and SD 1** is the most likely candidate for generating the observations 1 and -1 due to its highest likelihood value, which aligns well with both the data’s mean and variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLE: Linear Regression\n",
    "\n",
    "#### **Link Between Maximum Likelihood Estimation (MLE) and Linear Regression**\n",
    "\n",
    "In this explanation, you’ve demonstrated how Maximum Likelihood Estimation (MLE) can be applied to linear regression, showing that finding the line that best fits a set of data points is equivalent to maximizing the likelihood of observing those points given the line. Here’s a breakdown of the key concepts and steps:\n",
    "\n",
    "##### **1. MLE and Linear Regression:**\n",
    "- **Objective:** In linear regression, the goal is to find the line that best fits the given data points. This can be approached using MLE, where we model the data generation process probabilistically.\n",
    "\n",
    "##### **2. Data Generation Model:**\n",
    "- **Assumption:** We assume that the data points are generated from a Gaussian distribution centered around the line. For each point $x_i$, the Gaussian is centered at the vertical distance from $x_i$ to the line.\n",
    "\n",
    "##### **3. Likelihood Calculation:**\n",
    "- **Gaussian Likelihood:** For a given line $y = mx + b$, the distance $d_i$ from each data point $(x_i, y_i)$ to the line is calculated. The likelihood of observing each data point under the Gaussian distribution centered at the line can be written as:\n",
    "  $$P(y_i | x_i, \\text{line}) = \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\exp \\left( -\\frac{d_i^2}{2\\sigma^2} \\right)$$\n",
    "  where $\\sigma$ is the standard deviation of the Gaussian.\n",
    "\n",
    "- **Total Likelihood:** Since the data points are assumed to be independently generated, the total likelihood is the product of individual likelihoods:\n",
    "  $$L(\\text{line}) = \\prod_{i} P(y_i | x_i, \\text{line})$$\n",
    "  Taking the logarithm of the likelihood (log-likelihood) simplifies the multiplication into summation:\n",
    "  $$\\log L(\\text{line}) = \\sum_{i} \\log P(y_i | x_i, \\text{line})$$\n",
    "\n",
    "##### **4. Maximizing the Likelihood:**\n",
    "- **Simplified Expression:** Substituting the Gaussian likelihood into the log-likelihood equation:\n",
    "  $$\\log L(\\text{line}) = \\sum_{i} \\left( -\\frac{d_i^2}{2\\sigma^2} - \\log (\\sqrt{2 \\pi \\sigma^2}) \\right)$$\n",
    "  Since $\\log (\\sqrt{2 \\pi \\sigma^2})$ is a constant, it can be ignored in the optimization process:\n",
    "  $$\\log L(\\text{line}) \\propto -\\frac{1}{2\\sigma^2} \\sum_{i} d_i^2$$\n",
    "  Maximizing the likelihood is thus equivalent to minimizing the sum of squared distances $\\sum_{i} d_i^2$.\n",
    "\n",
    "- **Least Squares Error:** The objective function in linear regression is to minimize the sum of squared residuals (or errors), which is:\n",
    "  $$\\text{Sum of Squared Errors} = \\sum_{i} d_i^2$$\n",
    "\n",
    "  This is exactly what is done in linear regression.\n",
    "\n",
    "##### **5. Conclusion:**\n",
    "- **MLE and Linear Regression:** The line that maximizes the likelihood of the data being generated from that line, under the Gaussian assumption, is the same as the line that minimizes the least squares error. Hence, linear regression can be understood as a specific case of MLE where the data is assumed to be generated from a Gaussian distribution with the residuals centered around the line.\n",
    "\n",
    "#### **Example:**\n",
    "\n",
    "In the given example:\n",
    "- **Models:** Different lines are tested (Model 1, Model 2, Model 3).\n",
    "- **Evaluation:** Each line is evaluated by calculating the likelihood of the given data points assuming they are generated from a Gaussian centered at each line.\n",
    "- **Best Fit:** The line with the highest likelihood (or equivalently, the line with the smallest sum of squared distances) is selected as the best fit for the data.\n",
    "\n",
    "This illustrates how MLE provides a probabilistic framework for fitting models and connects directly to familiar regression techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularization\n",
    "\n",
    "#### **Regularization in the Context of Model Complexity and Maximum Likelihood**\n",
    "\n",
    "Regularization is a technique used to prevent overfitting by adding a penalty for model complexity. Here’s a breakdown of how it works and its connection to probability and maximum likelihood estimation (MLE).\n",
    "\n",
    "##### **1. Regularization Overview**\n",
    "\n",
    "- **Objective:** Regularization aims to improve the generalization of a model by discouraging overly complex models that might fit the training data too closely (overfitting). This is achieved by adding a penalty term to the loss function based on model complexity.\n",
    "\n",
    "##### **2. Types of Regularization**\n",
    "\n",
    "- **L2 Regularization (Ridge Regression):** Adds a penalty proportional to the sum of the squared coefficients (excluding the intercept term). The regularization term is given by:\n",
    "  $$\\text{Penalty} = \\lambda \\sum_{i=1}^n w_i^2$$\n",
    "  where $\\lambda$ is the regularization parameter, and $w_i$ are the model coefficients.\n",
    "\n",
    "- **Regularized Loss Function:** The total loss with regularization is:\n",
    "  $$\\text{Regularized Loss} = \\text{Log Loss} + \\lambda \\sum_{i=1}^n w_i^2$$\n",
    "\n",
    "##### **3. Example of Regularization**\n",
    "\n",
    "- **Models:**\n",
    "  - **Linear Model:** $y = 4x + 3$ with a loss of 10\n",
    "  - **Quadratic Model:** $y = 2x^2 - 4x + 5$ with a loss of 2\n",
    "  - **Polynomial Model of Degree 10:** with a loss of 0.1\n",
    "\n",
    "- **Penalties:**\n",
    "  - **Linear Model:** $4^2 = 16$\n",
    "  - **Quadratic Model:** $2^2 + (-4)^2 = 20$\n",
    "  - **Polynomial Model:** Sum of the squares of all coefficients, which equals 262\n",
    "\n",
    "- **Regularized Losses:**\n",
    "  - **Linear Model:** $10 + \\lambda \\times 16$\n",
    "  - **Quadratic Model:** $2 + \\lambda \\times 20$\n",
    "  - **Polynomial Model:** $0.1 + \\lambda \\times 262$\n",
    "\n",
    "  For a regularization parameter $\\lambda$, the regularized loss values adjust the fit. If $\\lambda$ is large enough, it increases the penalty for complex models, thus making simpler models more favorable.\n",
    "\n",
    "##### **4. Connection to Probability and MLE**\n",
    "\n",
    "- **MLE without Regularization:** Finds the model parameters that maximize the likelihood of the observed data. For complex models, this often leads to overfitting since the model can perfectly fit the training data without considering generalization.\n",
    "\n",
    "- **MLE with Regularization:** Incorporates regularization into the likelihood framework. The goal is to maximize the likelihood while also considering the penalty for model complexity. The regularization term helps to balance between fitting the data well and keeping the model simple.\n",
    "\n",
    "##### **5. How Regularization Affects Model Selection**\n",
    "\n",
    "- **Unregularized Selection:** Chooses the model with the lowest training error. In the absence of regularization, the complex polynomial (degree 10) might win due to its low error on the training data.\n",
    "\n",
    "- **Regularized Selection:** The addition of the regularization term adjusts the selection criteria, favoring models that are less complex even if they have slightly higher training error. The model selection is now influenced by both the fit to the data and the complexity penalty.\n",
    "\n",
    "##### **6. Summary**\n",
    "\n",
    "- **Regularization** is used to penalize model complexity, helping to select models that generalize better to unseen data.\n",
    "- **Incorporation into MLE:** Regularization modifies the standard MLE approach by adding a complexity penalty, leading to a trade-off between fit and simplicity.\n",
    "- **Practical Impact:** Regularization adjusts the loss function to prevent overfitting, thereby improving the model's ability to generalize to new data.\n",
    "\n",
    "Regularization helps ensure that the model you select is not only the best fit for the training data but also maintains a balance between complexity and generalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Back to \"Bayesics\"\n",
    "\n",
    "In your example, you're highlighting a fundamental concept in probability and Bayesian inference, where we need to consider both the likelihood of observing evidence given a scenario and the prior probability of that scenario itself. Let’s break down the concepts and how they relate to the example of popcorn on the floor.\n",
    "\n",
    "#### **Understanding the Example**\n",
    "\n",
    "##### **Initial Setup:**\n",
    "\n",
    "1. **Scenarios:**\n",
    "   - **Movies:** High probability of popcorn on the floor.\n",
    "   - **Board Games:** Medium probability of popcorn on the floor.\n",
    "   - **Nap:** Low probability of popcorn on the floor.\n",
    "\n",
    "2. **New Candidates:**\n",
    "   - **Movies:** High probability of popcorn on the floor.\n",
    "   - **Popcorn Throwing Contest:** Very high probability of popcorn on the floor.\n",
    "\n",
    "##### **Evaluating Scenarios:**\n",
    "\n",
    "1. **Probability of Evidence Given the Scenario:**\n",
    "   - **P(Popcorn | Movies)**: High\n",
    "   - **P(Popcorn | Contest)**: Very High\n",
    "\n",
    "2. **Prior Probability of the Scenarios:**\n",
    "   - **P(Movies)**: High\n",
    "   - **P(Contest)**: Low\n",
    "\n",
    "##### **Finding the Most Likely Scenario:**\n",
    "\n",
    "To find the most likely scenario, you should consider both:\n",
    "\n",
    "- **Likelihood of Evidence Given the Scenario** (how probable is the evidence if the scenario is true?)\n",
    "- **Prior Probability of the Scenario** (how likely is the scenario itself?)\n",
    "\n",
    "The correct approach is to maximize the **joint probability** of the evidence and the scenario. This involves multiplying the likelihood of the evidence given the scenario by the prior probability of the scenario:\n",
    "\n",
    "- **For Movies:**\n",
    "  $$  \\text{Joint Probability} = P(\\text{Popcorn} | \\text{Movies}) \\times P(\\text{Movies})$$\n",
    "\n",
    "- **For Contest:**\n",
    "  $$  \\text{Joint Probability} = P(\\text{Popcorn} | \\text{Contest}) \\times P(\\text{Contest})$$\n",
    "\n",
    "##### **Applying Bayes' Theorem:**\n",
    "\n",
    "Bayes’ Theorem helps in updating our beliefs about the scenarios based on the evidence:\n",
    "\n",
    "$$P(\\text{Movies} | \\text{Popcorn}) = \\frac{P(\\text{Popcorn} | \\text{Movies}) \\times P(\\text{Movies})}{P(\\text{Popcorn})}$$\n",
    "\n",
    "$$P(\\text{Contest} | \\text{Popcorn}) = \\frac{P(\\text{Popcorn} | \\text{Contest}) \\times P(\\text{Contest})}{P(\\text{Popcorn})}$$\n",
    "\n",
    "Here:\n",
    "\n",
    "- $P(\\text{Popcorn})$ is the overall probability of finding popcorn on the floor, regardless of the scenario.\n",
    "- The denominator $P(\\text{Popcorn})$ normalizes the probabilities so they sum to 1.\n",
    "\n",
    "##### **Why Movies Might Be Preferred:**\n",
    "\n",
    "Even though the contest has a very high likelihood of popcorn on the floor, its prior probability is very low. Thus, when considering both the likelihood and prior probability, movies might turn out to be the more probable scenario due to its higher prior probability, despite a slightly lower likelihood of generating popcorn.\n",
    "\n",
    "#### **Summary**\n",
    "\n",
    "- **Maximum Likelihood:** Looks at which scenario makes the observed evidence most probable but doesn’t consider how likely the scenario itself is.\n",
    "- **Bayesian Inference:** Considers both the likelihood of the evidence given the scenario and the prior probability of the scenario. It calculates the joint probability of evidence and scenario, updating beliefs in light of the evidence.\n",
    "\n",
    "In practice, considering both the likelihood and prior helps ensure that we choose the most realistic and plausible scenario, balancing evidence and prior knowledge effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian Statistics - Frequent vs. Bayesian\n",
    "\n",
    "The debate between frequentist and Bayesian approaches in statistics revolves around how probabilities are interpreted and used in inference. Let’s break down the key differences between these philosophies using your example of coin tossing:\n",
    "\n",
    "#### **Frequentist Approach:**\n",
    "\n",
    "1. **Probability Interpretation:**\n",
    "   - **Frequentists** view probability as the long-term frequency of an event occurring if an experiment is repeated an infinite number of times. For them, probability is objective and derived purely from data.\n",
    "   \n",
    "2. **Inference:**\n",
    "   - In your example, after tossing the coin 10 times and observing 8 heads and 2 tails, a frequentist would calculate the probability of heads as:\n",
    "     $$\\text{Probability of Heads} = \\frac{\\text{Number of Heads}}{\\text{Total Tosses}} = \\frac{8}{10} = 0.8$$\n",
    "   - This result is based entirely on the observed data, without incorporating any prior beliefs about the fairness of the coin.\n",
    "\n",
    "3. **Objective Evidence:**\n",
    "   - Frequentists rely on data collected from experiments or trials to make inferences. They use methods like Maximum Likelihood Estimation (MLE) to find the model parameters that make the observed data most likely, but they do not incorporate prior beliefs or external information.\n",
    "\n",
    "#### **Bayesian Approach:**\n",
    "\n",
    "1. **Probability Interpretation:**\n",
    "   - **Bayesians** view probability as a measure of belief or certainty about an event. This belief can be updated as new evidence is collected. Probabilities represent a degree of confidence in an event occurring.\n",
    "\n",
    "2. **Prior Beliefs:**\n",
    "   - Bayesians use **prior distributions** to incorporate prior knowledge or beliefs about a parameter before seeing the data. For instance, if the Bayesian initially believes the coin is likely fair, they might start with a prior belief that the probability of heads is around 0.5.\n",
    "\n",
    "3. **Updating Beliefs:**\n",
    "   - After observing the data (8 heads out of 10 tosses), Bayesians update their beliefs using **Bayes’ Theorem**:\n",
    "     $$P(\\text{Probability of Heads} | \\text{Data}) = \\frac{P(\\text{Data} | \\text{Probability of Heads}) \\times P(\\text{Probability of Heads})}{P(\\text{Data})}$$\n",
    "   - Here, \\(P(\\text{Probability of Heads})\\) is the prior belief, \\(P(\\text{Data} | \\text{Probability of Heads})\\) is the likelihood of the data given the probability of heads, and \\(P(\\text{Data})\\) is the overall probability of observing the data.\n",
    "\n",
    "4. **Posterior Distribution:**\n",
    "   - The result is a **posterior distribution**, which reflects updated beliefs about the probability of heads after considering the evidence. Even if the data shows 8 heads, the Bayesian might still assign a high probability to the coin being fair but will adjust the belief slightly based on the evidence.\n",
    "\n",
    "#### **Conceptual Differences:**\n",
    "\n",
    "- **Frequentist Methods:**\n",
    "  - **Model Selection:** Find the model that maximizes the likelihood of the observed data.\n",
    "  - **Parameter Estimation:** Focus on point estimates like MLE.\n",
    "\n",
    "- **Bayesian Methods:**\n",
    "  - **Model Selection:** Update prior beliefs based on observed data to get a posterior distribution.\n",
    "  - **Parameter Estimation:** Use the entire posterior distribution to make inferences, allowing for uncertainty in parameter estimates.\n",
    "\n",
    "#### **Illustrating with the Coin Toss Example:**\n",
    "\n",
    "- **Frequentist Result:** The probability of heads is estimated as 0.8 based on observed frequencies.\n",
    "  \n",
    "- **Bayesian Result:** The probability might be adjusted from the prior belief (0.5) to a value slightly different from 0.5, reflecting both the prior belief and the observed data. For example, if the prior was a fair coin and the observed evidence suggests 0.8, the Bayesian estimate might be a value like 0.65, reflecting the updated belief.\n",
    "\n",
    "In summary, while frequentists rely solely on observed data to make inferences, Bayesians incorporate prior beliefs and update these beliefs with new evidence. This difference leads to different approaches in handling uncertainty and making predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayasian Statistics - MAP\n",
    "\n",
    "\n",
    "#### **Different Priors and Their Impact**\n",
    "\n",
    "1. **Conservative Bayesian:**\n",
    "   - **Prior:** Very narrow, centered around 0.5.\n",
    "   - **Initial Belief:** Strongly believes the coin is fair.\n",
    "   - **Update After One Toss (Heads):** Minimal change in belief. The posterior is very close to the prior, reflecting that the Bayesian is highly skeptical of deviating from their initial belief.\n",
    "   - **Posterior Mode (MAP Estimation):** Close to 0.501, almost unchanged from the prior belief.\n",
    "\n",
    "2. **Moderate Bayesian:**\n",
    "   - **Prior:** Wider distribution, still centered around 0.5 but with more spread.\n",
    "   - **Initial Belief:** Believes the coin is likely fair but is open to some bias.\n",
    "   - **Update After One Toss (Heads):** Shows a noticeable shift in belief. The prior was more flexible, so the evidence has a more substantial impact on the posterior.\n",
    "   - **Posterior Mode (MAP Estimation):** Around 0.607, reflecting some adjustment from the initial belief but not as extreme.\n",
    "\n",
    "3. **Non-Informative Bayesian:**\n",
    "   - **Prior:** Uniform distribution, assigning equal probability to all possible values.\n",
    "   - **Initial Belief:** No strong prior belief; all outcomes are equally likely.\n",
    "   - **Update After One Toss (Heads):** Significant change in belief. Since the prior was non-informative, the posterior reflects the data more strongly.\n",
    "   - **Posterior Mode (MAP Estimation):** Around 0.8, which aligns with the frequentist estimate given the observed data.\n",
    "\n",
    "#### **Summary of Concepts:**\n",
    "\n",
    "- **Prior Distribution:**\n",
    "  - Represents initial beliefs before seeing the data.\n",
    "  - **Conservative Prior:** Strongly anchored belief; changes minimally.\n",
    "  - **Moderate Prior:** Allows for more flexibility; adjusts moderately.\n",
    "  - **Non-Informative Prior:** Equal belief across all possible outcomes; changes significantly based on data.\n",
    "\n",
    "- **Posterior Distribution:**\n",
    "  - Represents updated beliefs after observing data.\n",
    "  - **MAP Estimation:** The value of the parameter that maximizes the posterior distribution. It serves as the point estimate for the parameter considering both the prior and the data.\n",
    "\n",
    "- **Frequentist vs. Bayesian:**\n",
    "  - **Frequentist:** The result (MAP estimation) with an uninformative prior will match the frequentist estimate because it relies only on the observed data.\n",
    "  - **Bayesian:** The result depends on the prior belief. With informative priors, the posterior reflects both the prior and the evidence. \n",
    "\n",
    "#### **Key Takeaway:**\n",
    "\n",
    "Bayesian statistics allows for the incorporation of prior beliefs, which can significantly affect the results of statistical inference. The choice of prior impacts the posterior distribution and, consequently, the MAP estimation. In contrast, frequentist approaches are solely data-driven and do not incorporate prior beliefs.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian Statistics - Uploading Priors\n",
    "\n",
    "Bayes' theorem is a fundamental concept in Bayesian statistics, and understanding it is crucial for performing belief updates based on new evidence. Here's a summary of the key points from the video:\n",
    "\n",
    "### Bayes' Theorem Overview\n",
    "\n",
    "1. **Bayes' Theorem Formula**:\n",
    "   $$P(A \\mid B) = \\frac{P(B \\mid A) \\cdot P(A)}{P(B)}$$\n",
    "   - **Posterior ($P(A \\mid B)$)**: Updated probability of event A given evidence B.\n",
    "   - **Prior ($P(A)$)**: Initial probability of event A before considering evidence B.\n",
    "   - **Likelihood ($P(B \\mid A)$)**: Probability of evidence B given that event A is true.\n",
    "   - **Marginal Likelihood ($P(B)$)**: Total probability of evidence B, considering all possible scenarios.\n",
    "\n",
    "2. **Example**:\n",
    "   - **Event A**: Being offered a job.\n",
    "   - **Evidence B**: Receiving a request for a follow-up phone call.\n",
    "   - **Posterior**: Probability of being offered the job given the follow-up phone call.\n",
    "   - **Prior**: Initial belief about the likelihood of getting the job.\n",
    "   - **Likelihood**: Probability of receiving a follow-up call if you are to get the job.\n",
    "   - **Marginal Likelihood**: Overall probability of receiving a follow-up call.\n",
    "\n",
    "### Updating Beliefs with Bayesian Inference\n",
    "\n",
    "1. **Coin Example**:\n",
    "   - **Types of Coins**: Fair (0.5 heads) and Biased (0.8 heads).\n",
    "   - **Priors**: Initial belief about the coin's type (e.g., 75% fair, 25% biased).\n",
    "   - **Evidence**: Outcome of a coin flip (e.g., heads).\n",
    "\n",
    "2. **Steps to Update Beliefs**:\n",
    "   - **Calculate Posterior for Fair Coin**:\n",
    "     $$P(\\text{Fair} \\mid \\text{Heads}) = \\frac{P(\\text{Heads} \\mid \\text{Fair}) \\cdot P(\\text{Fair})}{P(\\text{Heads})}$$\n",
    "   - **Calculate Marginal Likelihood (Overall Probability of Heads)**:\n",
    "     $$P(\\text{Heads}) = P(\\text{Heads} \\mid \\text{Fair}) \\cdot P(\\text{Fair}) + P(\\text{Heads} \\mid \\text{Biased}) \\cdot P(\\text{Biased})$$\n",
    "   - **Update Priors**:\n",
    "     - **Posterior Probability**: Reflects updated belief after observing the evidence.\n",
    "     - **Example Calculation**: For 8 heads and 2 tails, updating beliefs results in:\n",
    "       - **Fair Coin**: Posterior probability might be updated to 65%.\n",
    "       - **Biased Coin**: Probability might increase to 35%.\n",
    "\n",
    "### Generalized Formula for Continuous and Discrete Variables\n",
    "\n",
    "1. **Discrete Discrete**:\n",
    "   $$P(Y \\mid X) = \\frac{P(X \\mid Y) \\cdot P(Y)}{P(X)}$$\n",
    "\n",
    "2. **Continuous Continuous**:\n",
    "   $$f(Y \\mid X) = \\frac{f(X \\mid Y) \\cdot f(Y)}{f(X)}$$\n",
    "\n",
    "3. **Discrete Continuous**:\n",
    "   $$P(Y \\mid X) = \\frac{f(X \\mid Y) \\cdot P(Y)}{f(X)}$$\n",
    "\n",
    "4. **Continuous Discrete**:\n",
    "   $$f(Y \\mid X) = \\frac{P(X \\mid Y) \\cdot f(Y)}{f(X)}$$\n",
    "\n",
    "### Maximum A Posteriori (MAP) Estimation\n",
    "\n",
    "- **MAP Estimation**: To determine the most likely value of the parameter after observing the evidence, you use the mode of the posterior distribution.\n",
    "\n",
    "### Summary\n",
    "\n",
    "Bayes' theorem provides a systematic way to update beliefs about an event or parameter based on new evidence. By calculating the posterior probability, you refine your initial beliefs (priors) and can make more informed decisions or predictions. The process involves considering the likelihood of the evidence, updating priors accordingly, and using various formulas depending on whether the variables are discrete or continuous.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian Statistics - Full Worked Example \n",
    "\n",
    "\n",
    "### Bayesian Statistics Summary and Notes\n",
    "\n",
    "#### **Overview:**\n",
    "Bayesian statistics is a method of statistical inference where prior beliefs are updated with new data. The core concept involves revising the probability of a hypothesis based on observed evidence. This process uses Bayes' theorem to update the prior distribution to a posterior distribution.\n",
    "\n",
    "#### **Key Concepts:**\n",
    "\n",
    "1. **Bayes' Theorem:**\n",
    "   Bayes' theorem allows us to update our beliefs about a probability based on new evidence.\n",
    "   $$P(\\theta | x) = \\frac{P(x | \\theta) \\cdot P(\\theta)}{P(x)}$$\n",
    "   where:\n",
    "   - $P(\\theta | x)$ is the posterior probability of $\\theta$ given the data $x$.\n",
    "   - $P(x | \\theta)$ is the likelihood of the data given $\\theta$.\n",
    "   - $P(\\theta)$ is the prior probability of $\\theta$.\n",
    "   - $P(x)$ is the marginal likelihood of the data.\n",
    "\n",
    "2. **Setting Up the Problem:**\n",
    "   - **Define $\\theta$**: Random variable representing the probability of heads in a coin flip.\n",
    "   - **Data Representation**: Collect data $x = (x_1, x_2, ..., x_{10})$, where each $x_i$ is a Bernoulli random variable (1 for heads, 0 for tails).\n",
    "\n",
    "3. **Likelihood Calculation:**\n",
    "   - For each flip, the likelihood is given by $\\theta^k \\cdot (1 - \\theta)^{n - k}$, where $k$ is the number of heads and $n$ is the total number of flips.\n",
    "   - Example: For 8 heads and 2 tails, the likelihood is $\\theta^8 \\cdot (1 - \\theta)^2$.\n",
    "\n",
    "4. **Choosing Priors:**\n",
    "   - If no prior information is available, use a uniform prior: $P(\\theta) = 1$ for $\\theta$ in [0,1].\n",
    "   - This prior is often represented as a Beta distribution with parameters $\\alpha = 1$ and $\\beta = 1$, i.e., Beta(1,1).\n",
    "\n",
    "5. **Posterior Distribution:**\n",
    "   - Combine the likelihood with the prior to get the posterior distribution:\n",
    "   $$P(\\theta | x) \\propto \\theta^k \\cdot (1 - \\theta)^{n - k} \\cdot P(\\theta)$$\n",
    "   - For a uniform prior, the posterior distribution will be Beta($k+1$, $n-k+1$).\n",
    "\n",
    "6. **Updating Beliefs with New Data:**\n",
    "   - After the initial data (e.g., 10 flips), the posterior becomes the new prior for subsequent updates.\n",
    "   - Example: After 10 flips (8 heads, 2 tails) with a Beta(1,1) prior, the posterior is Beta(9, 3). If 10 more flips yield 6 heads and 4 tails, update the prior to Beta(9,3) and calculate the new posterior as Beta(15,7).\n",
    "\n",
    "7. **Maximum A Posteriori (MAP) Estimation:**\n",
    "   - The MAP estimate is the mode of the posterior distribution.\n",
    "   - For a Beta distribution Beta($\\alpha$, $\\beta$), the MAP estimate is:\n",
    "   $$\\frac{\\alpha - 1}{\\alpha + \\beta - 2}$$\n",
    "   - Example: For Beta(9, 3), MAP estimate is $\\frac{8}{11} \\approx 0.73$.\n",
    "\n",
    "8. **Frequentist vs. Bayesian Approach:**\n",
    "   - Frequentist methods focus on the data alone and ignore prior distributions.\n",
    "   - Bayesian methods incorporate prior beliefs and update them with new data.\n",
    "   - With large amounts of data, Bayesian and frequentist results converge, but Bayesian methods are preferred when prior information is crucial or data is limited.\n",
    "\n",
    "#### **Important Points:**\n",
    "\n",
    "- **Beta Distribution**: Conjugate prior for Bernoulli likelihood; posterior is also a Beta distribution.\n",
    "- **Constant Terms**: Often ignored in practical calculations, as they do not affect the shape or the MAP estimate.\n",
    "- **Updating Priors**: Iteratively update priors with new data to refine beliefs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relationship between MAP, MLE, and Regularization\n",
    "\n",
    "1. **Maximum Likelihood Estimation (MLE)**: \n",
    "   - MLE aims to find the model parameters that maximize the probability of the observed data given the model. For example, if you have multiple models, you choose the one that best explains the data.\n",
    "\n",
    "2. **Regularization**:\n",
    "   - Regularization helps prevent overfitting by penalizing complex models. This is done by adding a term to the loss function that discourages large coefficients.\n",
    "\n",
    "3. **Combining MLE and Regularization**:\n",
    "   - When you incorporate regularization into MLE, you adjust the likelihood with a prior probability distribution over the model parameters. This can be seen as combining the likelihood of the data given the model with the probability of the model itself.\n",
    "   - This combination leads to a loss function that includes both the fit of the model to the data (e.g., squared loss) and a regularization term (e.g., L2 norm of coefficients).\n",
    "\n",
    "4. **Mathematical Derivation**:\n",
    "   - To combine MLE with regularization, you transform the product of probabilities into a sum by taking logarithms. This converts the problem into one of minimizing a loss function that includes both the data fitting term (like squared loss) and a regularization term.\n",
    "   - For example, if the model parameters are drawn from a normal distribution, the likelihood is based on the product of these probabilities. The log transformation simplifies this to a sum of squared terms, representing the regularization term.\n",
    "\n",
    "5. **Final Model Selection**:\n",
    "   - The final model is selected by maximizing the posterior probability of the model given the data, which translates into minimizing the regularized loss function. This combines minimizing the sum of squared errors (fit) with the regularization term.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
